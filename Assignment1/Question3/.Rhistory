hist(precip, breaks = 200, main = "")
?hist
## Uniform Distribution ###
?runif
# generating a random number
runif(1)
#generating 3 random numbers
runif(3)
# generating 3 random numbers on a defined range
runif(3, min=5, max=10)
runif(5, min=0.25, max=0.5)
#generating an uniform random vector of 100 observations for the interval [0,1]
vec_u<-runif(100,0,1)
hist(vec_u)
#generating an uniform random vector of 100 observations for the interval [0,1]
vec_u<-runif(10000,0,1)
hist(vec_u)
# Download the laptop prices data set from the web site of kaggle
# (Laptop Price Prediction Cleaned Dataset) and read the data set in R.
# The data set consists of following variables: Company, Type Name, Ram, Weight, Price, # nolint: line_length_linter.
#    Touch Screen indicator, IPS indicator, Ppi, Cpu_brand, HDD, SSD, Gpu_brand, Os (Operation System). # nolint
# a) Import data set to R assigning the type of each variable correctly. (5p)
# b) Create a dataset including only types of laptops: Ultrabook, Notebook and 2 in 1 Convertible. (5p)
# c) Summarize the variables Weight, Price and the categorical variables in the new created data set. (10p)
# d) Cross classify the variables type of computer and the touch screen indicator in a table. Compute and interpret the conditional probability tables. (15p)
# e) Is there an association between the type of computer and the touch screen characteristic of a computer. Analyze it by using proper statistical method. (10p)
# f) Check the distribution of Price first for all observations then for subgroups of type of laptop in the data set created in section (b).
#     Does it follow a normal distribution? (15p)
# g) Create a data frame just by including Ultrabooks and Notebooks. (5p)
# h) Make a boxplot to show the distribution of Price across two categories of Type: Ultrabook vs. Notebook. Interpret it. (10p)
# i) Compare the average price of Ultrabooks and Notebooks by using the appropriate method. Do not forget to test the assumptions. (25p)
echo = TRUE
# a) Import data set to R assigning the type of each variable correctly. (5p)
laptop_prices <- read.csv("C:\\chomsky\\Academics\\Spring-2024\\SMDE\\Project\\Project1\\laptop_data_cleaned.csv", header = TRUE, sep = ",")
# b) Create a dataset including only types of laptops: Ultrabook, Notebook and 2 in 1 Convertible. (5p)
laptop_prices_filtered <- laptop_prices[laptop_prices$TypeName %in% c("Ultrabook", "Notebook", "2 in 1 Convertible"), ]
print(head(laptop_prices_filtered))
#print(head(laptop_prices_filtered))
# c) Summarize the variables Weight, Price and the categorical variables in the new created data set. (10p)
print("Summary of the variables in the new created data set:")
# 1. Identify Columns to Convert
columns_to_convert <- c("Company", "TypeName", "TouchScreen", "Ips", "Cpu_brand", "Gpu_brand", "Os", "HDD", "SSD", "Ram", "Ppi")
for (column_name in columns_to_convert) {
laptop_prices_filtered[, column_name] <- factor(laptop_prices_filtered[, column_name])
}
print(summary(laptop_prices_filtered))
# d) Cross classify the variables type of computer and the touch screen indicator in a table. Compute and interpret the conditional probability tables. (15p)
# Replace 1 with "Touch Screen" and 0 with "No Touch Screen"
laptop_prices_filtered$TouchScreen <- factor(laptop_prices_filtered$TouchScreen,
levels = c(0, 1),
labels = c("Not Touch Screen", "Touch Screen"))
# Cross-classification table
print("Probability table for TypeName and TouchScreen:")
print(table(laptop_prices_filtered$TypeName, laptop_prices_filtered$TouchScreen))
# Conditional probability tables
print(prop.table(table(laptop_prices_filtered$TypeName, laptop_prices_filtered$TouchScreen), 1))
print(prop.table(table(laptop_prices_filtered$TypeName, laptop_prices_filtered$TouchScreen), 2))
# e) Is there an association between the type of computer and the touch screen characteristic of a computer. Analyze it by using proper statistical method. (10p)
print("Chi-square test for TypeName and TouchScreen:")
print(chisq.test(table(laptop_prices_filtered$TypeName, laptop_prices_filtered$TouchScreen)))
# f) Check the distribution of Price first for all observations then for subgroups of type of laptop in the data set created in section (b).
#     Does it follow a normal distribution? (15p)
#Histogram for all observations
print("Shapiro-Wilk test for Price for all observations:")
hist(laptop_prices_filtered$Price, main = "Histogram of Price for all observations", xlab = "Price", ylab = "Frequency", col = "lightblue")
print(shapiro.test(laptop_prices_filtered$Price))
print("Shapiro-Wilk test for Price for subgroups of type of laptop:")
#Histogram for subgroups of type of laptop
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Ultrabook"], main = "Histogram of Price for Ultrabooks", xlab = "Price", ylab = "Frequency", col = "lightblue")
print(shapiro.test(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Ultrabook"]))
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Notebook"], main = "Histogram of Price for Notebooks", xlab = "Price", ylab = "Frequency", col = "lightblue")
print(shapiro.test(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Notebook"]))
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "2 in 1 Convertible"], main = "Histogram of Price for 2 in 1 Convertibles", xlab = "Price", ylab = "Frequency", col = "lightblue")
print(shapiro.test(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "2 in 1 Convertible"]))
# g) Create a data frame just by including Ultrabooks and Notebooks. (5p)
laptop_prices_filtered_2 <- laptop_prices_filtered[laptop_prices_filtered$TypeName %in% c("Ultrabook", "Notebook"), ]
laptop_prices_filtered_2$TypeName <- factor(laptop_prices_filtered_2$TypeName,
levels = c("Ultrabook", "Notebook")) # Specify only desired levels
# Now create the boxplot
boxplot(Price ~ TypeName, data = laptop_prices_filtered_2)
# h) Make a boxplot to show the distribution of Price across two categories of Type: Ultrabook vs. Notebook. Interpret it. (10p)
boxplot(Price ~ TypeName, data = laptop_prices_filtered_2)
# i) Compare the average price of Ultrabooks and Notebooks by using the appropriate method. Do not forget to test the assumptions. (25p)
library(car)
print("Levene's test for Price between Ultrabooks and Notebooks:")
print(leveneTest(Price ~ TypeName, data = laptop_prices_filtered_2) )
print("T-test for Price between Ultrabooks and Notebooks:")
print(t.test(Price ~ TypeName, data = laptop_prices_filtered_2))
hist(laptop_prices_filtered$Price, main = "Histogram of Price for all observations", xlab = "Price", ylab = "Frequency", col = "lightblue")
#Histogram for subgroups of type of laptop
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Ultrabook"], main = "Histogram of Price for Ultrabooks", xlab = "Price", ylab = "Frequency", col = "lightblue")
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Notebook"], main = "Histogram of Price for Notebooks", xlab = "Price", ylab = "Frequency", col = "lightblue")
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "2 in 1 Convertible"], main = "Histogram of Price for 2 in 1 Convertibles", xlab = "Price", ylab = "Frequency", col = "lightblue")
# h) Make a boxplot to show the distribution of Price across two categories of Type: Ultrabook vs. Notebook. Interpret it. (10p)
boxplot(Price ~ TypeName, data = laptop_prices_filtered_2)
# Download the laptop prices data set from the web site of kaggle
# (Laptop Price Prediction Cleaned Dataset) and read the data set in R.
# The data set consists of following variables: Company, Type Name, Ram, Weight, Price, # nolint: line_length_linter.
#    Touch Screen indicator, IPS indicator, Ppi, Cpu_brand, HDD, SSD, Gpu_brand, Os (Operation System). # nolint
# a) Import data set to R assigning the type of each variable correctly. (5p)
# b) Create a dataset including only types of laptops: Ultrabook, Notebook and 2 in 1 Convertible. (5p)
# c) Summarize the variables Weight, Price and the categorical variables in the new created data set. (10p)
# d) Cross classify the variables type of computer and the touch screen indicator in a table. Compute and interpret the conditional probability tables. (15p)
# e) Is there an association between the type of computer and the touch screen characteristic of a computer. Analyze it by using proper statistical method. (10p)
# f) Check the distribution of Price first for all observations then for subgroups of type of laptop in the data set created in section (b).
#     Does it follow a normal distribution? (15p)
# g) Create a data frame just by including Ultrabooks and Notebooks. (5p)
# h) Make a boxplot to show the distribution of Price across two categories of Type: Ultrabook vs. Notebook. Interpret it. (10p)
# i) Compare the average price of Ultrabooks and Notebooks by using the appropriate method. Do not forget to test the assumptions. (25p)
echo = TRUE
# a) Import data set to R assigning the type of each variable correctly. (5p)
laptop_prices <- read.csv("C:\\chomsky\\Academics\\Spring-2024\\SMDE\\Project\\Project1\\laptop_data_cleaned.csv", header = TRUE, sep = ",")
# b) Create a dataset including only types of laptops: Ultrabook, Notebook and 2 in 1 Convertible. (5p)
laptop_prices_filtered <- laptop_prices[laptop_prices$TypeName %in% c("Ultrabook", "Notebook", "2 in 1 Convertible"), ]
print(head(laptop_prices_filtered))
#print(head(laptop_prices_filtered))
# c) Summarize the variables Weight, Price and the categorical variables in the new created data set. (10p)
print("Summary of the variables in the new created data set:")
# 1. Identify Columns to Convert
columns_to_convert <- c("Company", "TypeName", "TouchScreen", "Ips", "Cpu_brand", "Gpu_brand", "Os", "HDD", "SSD", "Ram", "Ppi")
for (column_name in columns_to_convert) {
laptop_prices_filtered[, column_name] <- factor(laptop_prices_filtered[, column_name])
}
print(summary(laptop_prices_filtered))
# d) Cross classify the variables type of computer and the touch screen indicator in a table. Compute and interpret the conditional probability tables. (15p)
# Replace 1 with "Touch Screen" and 0 with "No Touch Screen"
laptop_prices_filtered$TouchScreen <- factor(laptop_prices_filtered$TouchScreen,
levels = c(0, 1),
labels = c("Not Touch Screen", "Touch Screen"))
# Cross-classification table
print("Probability table for TypeName and TouchScreen:")
print(table(laptop_prices_filtered$TypeName, laptop_prices_filtered$TouchScreen))
# Conditional probability tables
print(prop.table(table(laptop_prices_filtered$TypeName, laptop_prices_filtered$TouchScreen), 1))
print(prop.table(table(laptop_prices_filtered$TypeName, laptop_prices_filtered$TouchScreen), 2))
# e) Is there an association between the type of computer and the touch screen characteristic of a computer. Analyze it by using proper statistical method. (10p)
print("Chi-square test for TypeName and TouchScreen:")
print(chisq.test(table(laptop_prices_filtered$TypeName, laptop_prices_filtered$TouchScreen)))
# f) Check the distribution of Price first for all observations then for subgroups of type of laptop in the data set created in section (b).
#     Does it follow a normal distribution? (15p)
#Histogram for all observations
print("Shapiro-Wilk test for Price for all observations:")
hist(laptop_prices_filtered$Price, main = "Histogram of Price for all observations", xlab = "Price", ylab = "Frequency", col = "lightblue")
print(shapiro.test(laptop_prices_filtered$Price))
print("Shapiro-Wilk test for Price for subgroups of type of laptop:")
#Histogram for subgroups of type of laptop
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Ultrabook"], main = "Histogram of Price for Ultrabooks", xlab = "Price", ylab = "Frequency", col = "lightblue")
print(shapiro.test(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Ultrabook"]))
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Notebook"], main = "Histogram of Price for Notebooks", xlab = "Price", ylab = "Frequency", col = "lightblue")
print(shapiro.test(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "Notebook"]))
hist(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "2 in 1 Convertible"], main = "Histogram of Price for 2 in 1 Convertibles", xlab = "Price", ylab = "Frequency", col = "lightblue")
print(shapiro.test(laptop_prices_filtered$Price[laptop_prices_filtered$TypeName == "2 in 1 Convertible"]))
# g) Create a data frame just by including Ultrabooks and Notebooks. (5p)
laptop_prices_filtered_2 <- laptop_prices_filtered[laptop_prices_filtered$TypeName %in% c("Ultrabook", "Notebook"), ]
laptop_prices_filtered_2$TypeName <- factor(laptop_prices_filtered_2$TypeName,
levels = c("Ultrabook", "Notebook")) # Specify only desired levels
# Now create the boxplot
boxplot(Price ~ TypeName, data = laptop_prices_filtered_2)
# h) Make a boxplot to show the distribution of Price across two categories of Type: Ultrabook vs. Notebook. Interpret it. (10p)
boxplot(Price ~ TypeName, data = laptop_prices_filtered_2)
# i) Compare the average price of Ultrabooks and Notebooks by using the appropriate method. Do not forget to test the assumptions. (25p)
library(car)
print("Levene's test for Price between Ultrabooks and Notebooks:")
print(leveneTest(Price ~ TypeName, data = laptop_prices_filtered_2) )
print("T-test for Price between Ultrabooks and Notebooks:")
print(t.test(Price ~ TypeName, data = laptop_prices_filtered_2))
#set working directory to current file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(car) # For assumption testing
laptop_data <- read.csv("laptop_data_cleaned.csv", header = TRUE, sep = ",")
print(head(laptop_data))
# Use the full laptop prices data set from the web site of kaggle (Laptop Price Prediction Cleaned Dataset).
# The data set consists of following variables: Company, Type Name, Ram, Weight, Price, Touch Screenindicator, IPSindicator,
# Ppi, Cpu_brand, HDD, SSD, Gpu_brand, Os (Operation System).
# You had already assigned the type of the variables. Now fit a linear regression model to predict price of laptops.
# a)Consider  the  numerical  variables  in  the  data  set  and find  the  best  simple  linear  regression model to predict
# the prices (Test  the assumptions and use  transformations if it is required.)
# Explain why the model you find is the best simple linear regression model and interpret the coefficients of the model (25p)
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
# Using Ram as it has the highest correlation with Price, had to remove outlier of 64GB
# transform the Ram variable using log transformation
laptop_data$Ram <- log(laptop_data$Ram)
model_ram <- lm(Price ~ Ram, data = laptop_data)
print((summary(model_ram) ))
print(shapiro.test(model_ram$residuals) )# Shapiro-Wilk Test
# Assumptions:
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model_ram)
#hist(model_ram$residuals)   # Histogram
#plot price vs ram
plot(laptop_data$Ram, laptop_data$Price, main = "Price vs Ram", xlab = "Ram", ylab = "Price", pch = 19, col = "blue")
# Fit the simple linear regression model
#lm_model <- lm(Price ~ ., data = numerical_vars)
# Check the assumptions of the linear regression model
# You can use diagnostic plots like residuals vs. fitted values, normal Q-Q plot, and scale-location plot to assess the assumptions.
# Interpret the coefficients of the model
#summary(lm_model)
# b) Fit a multivariate linear regression model with two (numerical) independent variables. Choose the most significant regression
# model with two predictors. (Transform the variables if it is needed and test all the assumptions.)
# Then compare this model to the simple linear regression model that you fit in (a). Which one is a better model? Why? (25p)
# Select the two most significant predictors
#best_model <- lm(Price ~ Ram + Weight, data = numerical_vars)
# Check the assumptions of the multivariate linear regression model
# Compare the models using metrics like R-squared, adjusted R-squared, and AIC/BIC values
# c) Now add a factor to the regression model you have chosen in section (b). (You can write a loop to add factors one by one to
# the previous model and decide based on the results.) Interpret the coefficients and overall summary of the model.
# Test the model in section (b) with the model that has an additional factor. Which one would you choose? Why? (35p)
# Add a factor variable to the model
# Interpret the coefficients and overall summary of the model
# Compare the models using metrics like R-squared, adjusted R-squared, and AIC/BIC values
# d) Test the validity of the final model. (15p)
# Perform hypothesis tests on the coefficients to assess their significance
# Check the overall significance of the model using an F-test
# Evaluate the goodness of fit of the model using metrics like R-squared, adjusted R-squared, and AIC/BIC values
# Perform diagnostic checks on the model assumptions
plot(model_ram)
Assumptions:
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
Assumptions:
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model_ram)
# a)Consider  the  numerical  variables  in  the  data  set  and find  the  best  simple  linear  regression model to predict
# the prices (Test  the assumptions and use  transformations if it is required.)
# Explain why the model you find is the best simple linear regression model and interpret the coefficients of the model (25p)
laptop_data$Ram <- log(laptop_data$Ram)
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
laptop_data$Ppi <- log(laptop_data$Ppi)
# after log transformation, the Ram variable is normally distributed and better correlated with the Price variable
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
print(corMat["Ppi", ])
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
laptop_data$SSD <- log(laptop_data$SSD)
# after log transformation, the Ram variable is normally distributed and better correlated with the Price variable
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
#set working directory to current file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(car) # For assumption testing
laptop_data <- read.csv("laptop_data_cleaned.csv", header = TRUE, sep = ",")
# a)Consider  the  numerical  variables  in  the  data  set  and find  the  best  simple  linear  regression model to predict
# the prices (Test  the assumptions and use  transformations if it is required.)
# Explain why the model you find is the best simple linear regression model and interpret the coefficients of the model (25p)
#laptop_data$Ram <- log(laptop_data$Ram)
laptop_data$Ppi <- log(laptop_data$Ppi)
# Apply log reansfomation to SSD and turn 0 to 1
laptop_data$SSD <- log(laptop_data$SSD + 1)
# after log transformation, the Ram variable is normally distributed and better correlated with the Price variable
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
plot(laptop_data$Price, laptop_data$Ram)
plot(laptop_data$Ram, laptop_data$Price )
# a)Consider  the  numerical  variables  in  the  data  set  and find  the  best  simple  linear  regression model to predict
# the prices (Test  the assumptions and use  transformations if it is required.)
# Explain why the model you find is the best simple linear regression model and interpret the coefficients of the model (25p)
laptop_data$Ram <- log(laptop_data$Ram)
laptop_data$Ppi <- log(laptop_data$Ppi)
# Apply log reansfomation to SSD and turn 0 to 1
laptop_data$SSD <- log(laptop_data$SSD + 1)
# after log transformation, the Ram variable is normally distributed and better correlated with the Price variable
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
# transform the Ram variable using log transformation
model_ram <- lm(Price ~ Ram, data = laptop_data)
print((summary(model_ram) ))
print(shapiro.test(model_ram$residuals) )# Shapiro-Wilk Test
durbinWatsonTest(model_ram)
laptop_data <- read.csv("laptop_data_cleaned.csv", header = TRUE, sep = ",")
# a)Consider  the  numerical  variables  in  the  data  set  and find  the  best  simple  linear  regression model to predict
# the prices (Test  the assumptions and use  transformations if it is required.)
# Explain why the model you find is the best simple linear regression model and interpret the coefficients of the model (25p)
#laptop_data$Ram <- log(laptop_data$Ram)
laptop_data$Ppi <- log(laptop_data$Ppi)
# Apply log reansfomation to SSD and turn 0 to 1
laptop_data$SSD <- log(laptop_data$SSD + 1)
# after log transformation, the Ram variable is normally distributed and better correlated with the Price variable
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
# transform the Ram variable using log transformation
model_ram <- lm(Price ~ Ram, data = laptop_data)
print((summary(model_ram) ))
print(shapiro.test(model_ram$residuals) )# Shapiro-Wilk Test
durbinWatsonTest(model_ram)
library(car) # For assumption testing
laptop_data <- read.csv("laptop_data_cleaned.csv", header = TRUE, sep = ",")
# a)Consider  the  numerical  variables  in  the  data  set  and find  the  best  simple  linear  regression model to predict
# the prices (Test  the assumptions and use  transformations if it is required.)
# Explain why the model you find is the best simple linear regression model and interpret the coefficients of the model (25p)
laptop_data$Ram <- log(laptop_data$Ram)
laptop_data$Ppi <- log(laptop_data$Ppi)
# Apply log reansfomation to SSD and turn 0 to 1
laptop_data$SSD <- log(laptop_data$SSD + 1)
# after log transformation, the Ram variable is normally distributed and better correlated with the Price variable
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
# transform the Ram variable using log transformation
model_ram <- lm(Price ~ Ram, data = laptop_data)
print((summary(model_ram) ))
print(shapiro.test(model_ram$residuals) )# Shapiro-Wilk Test
durbinWatsonTest(model_ram)
# a)Consider  the  numerical  variables  in  the  data  set  and find  the  best  simple  linear  regression model to predict
# the prices (Test  the assumptions and use  transformations if it is required.)
# Explain why the model you find is the best simple linear regression model and interpret the coefficients of the model (25p)
laptop_data$Ram <- log(laptop_data$Ram)
laptop_data$Ppi <- log(laptop_data$Ppi)
# Apply log reansfomation to SSD and turn 0 to 1
laptop_data$SSD <- log(laptop_data$SSD + 1)
# after log transformation, the Ram variable is normally distributed and better correlated with the Price variable
# Correlation between price and numerical variables.
corMat <- laptop_data %>%
select(Price, where(is.numeric)) %>%
cor(use = "pairwise.complete.obs")
# Get the correlation of the Price variable with the other numerical variables
print(corMat["Price", ]) # Theres is a cprrelation between Price and Ram,
# transform the Ram variable using log transformation
model_ram <- lm(Price ~ Ram, data = laptop_data)
print((summary(model_ram) ))
print(shapiro.test(model_ram$residuals) )# Shapiro-Wilk Test
durbinWatsonTest(model_ram)
#scatter plot tells us that the relationship between price and ram is linear
# residual vs fitted plot tells us that the residuals are homoscedastic
# histogram tells us that the residuals are normally distributed
# qq plot tells us that the residuals are normally distributed
# Assumptions:
#par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model_ram)
#scatter plot tells us that the relationship between price and ram is linear
# residual vs fitted plot tells us that the residuals are homoscedastic
# histogram tells us that the residuals are normally distributed
# qq plot tells us that the residuals are normally distributed
# Assumptions:
#par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model_ram)
leveneTest(model_ram) # test for homoscedta
# test for linearity of the relationship between the dependent and independent variables using a scatter plot
plot(laptop_data$Ram, laptop_data$Price, main = "Price vs Log(Ram)", xlab = "Log(Ram)", ylab = "Price", pch = 19, col = "blue")
# add regression line to the scatter plot
abline(model_ram, col = "red")
print((summary(model_ram) ))
# A list of potential predictors (modify as needed based on your data)
potential_predictors <- c("log(Ram)", "Weight", "SSD", "Ppi")
# Remove the dependent variable ('Price') from the list
potential_predictors <- potential_predictors[!potential_predictors == "Price"]
# Function to build a model, check assumptions, and store results (modify as needed)
build_and_assess_model <- function(formula, data) {
model <- lm(formula, data = data)
# Add assumption checks here using 'plot(model)'...
return(c(adjusted.r.squared(model), AIC(model), BIC(model)))
}
# Empty container to store model results
model_results <- matrix(nrow = choose(length(potential_predictors), 2),
ncol = 4,
dimnames = list(NULL, c("Combination", "Adj. R-squared", "AIC", "BIC")))
# Loop through combinations of two predictors
counter <- 1
for (i in 1:(length(potential_predictors) - 1)) {
for (j in (i + 1):length(potential_predictors)) {
formula <- as.formula(paste("Price ~", potential_predictors[i], "+", potential_predictors[j]))
model_results[counter,] <- c(paste(potential_predictors[i], "+", potential_predictors[j]), build_and_assess_model(formula, data = laptop_data))
counter <- counter + 1
}
}
# Function to build a model, check assumptions, and store results (modify as needed)
build_and_assess_model <- function(formula, data) {
model <- lm(formula, data = data)
# Add assumption checks here using 'plot(model)'...
return(c(adjusted.r.squared(model), AIC(model), BIC(model)))
}
# Empty container to store model results
model_results <- matrix(nrow = choose(length(potential_predictors), 2),
ncol = 4,
dimnames = list(NULL, c("Combination", "Adj. R-squared", "AIC", "BIC")))
# Loop through combinations of two predictors
counter <- 1
for (i in 1:(length(potential_predictors) - 1)) {
for (j in (i + 1):length(potential_predictors)) {
formula <- as.formula(paste("Price ~", potential_predictors[i], "+", potential_predictors[j]))
model_results[counter,] <- c(paste(potential_predictors[i], "+", potential_predictors[j]), build_and_assess_model(formula, data = laptop_data))
counter <- counter + 1
}
}
library(lmtest)
install.packages(lmtest)
install.package(lmtest)
#set working directory to current file location
#install lmtest package
install.packages("lmtest")
# b) Fit a multivariate linear regression model with two (numerical) independent variables. Choose the most significant regression
# model with two predictors. (Transform the variables if it is needed and test all the assumptions.)
# Then compare this model to the simple linear regression model that you fit in (a). Which one is a better model? Why? (25p)
# Assuming 'laptop_data' has your data and 'Price' is the dependent variable
library(lmtest)
# A list of potential predictors (modify as needed based on your data)
potential_predictors <- c("Ram", "Weight", "SSD", "Ppi")
# Remove the dependent variable ('Price') from the list
potential_predictors <- potential_predictors[!potential_predictors == "Price"]
# Function to build a model, check assumptions, and store results (modify as needed)
build_and_assess_model <- function(formula, data) {
model <- lm(formula, data = data)
# Add assumption checks here using 'plot(model)'...
return(c(adjusted.r.squared(model), AIC(model), BIC(model)))
}
# Empty container to store model results
model_results <- matrix(nrow = choose(length(potential_predictors), 2),
ncol = 4,
dimnames = list(NULL, c("Combination", "Adj. R-squared", "AIC", "BIC")))
# Loop through combinations of two predictors
counter <- 1
for (i in 1:(length(potential_predictors) - 1)) {
for (j in (i + 1):length(potential_predictors)) {
formula <- as.formula(paste("Price ~", potential_predictors[i], "+", potential_predictors[j]))
model_results[counter,] <- c(paste(potential_predictors[i], "+", potential_predictors[j]), build_and_assess_model(formula, data = laptop_data))
counter <- counter + 1
}
}
library(lmtest)
# A list of potential predictors (modify as needed based on your data)
potential_predictors <- c("Ram", "Weight", "SSD", "Ppi")
# Remove the dependent variable ('Price') from the list
potential_predictors <- potential_predictors[!potential_predictors == "Price"]
# Function to build a model, check assumptions, and store results (modify as needed)
build_and_assess_model <- function(formula, data) {
model <- lm(formula, data = data)
# Add assumption checks here using 'plot(model)'...
return(c(adjusted.r.squared(model), AIC(model), BIC(model)))
}
# Empty container to store model results
model_results <- matrix(nrow = choose(length(potential_predictors), 2),
ncol = 4,
dimnames = list(NULL, c("Combination", "Adj. R-squared", "AIC", "BIC")))
# Loop through combinations of two predictors
counter <- 1
for (i in 1:(length(potential_predictors) - 1)) {
for (j in (i + 1):length(potential_predictors)) {
formula <- as.formula(paste("Price ~", potential_predictors[i], "+", potential_predictors[j]))
model_results[counter,] <- c(paste(potential_predictors[i], "+", potential_predictors[j]), build_and_assess_model(formula, data = laptop_data))
counter <- counter + 1
}
}
library(lmtest)
for (i in 1:(length(potential_predictors) - 1)) {
for (j in (i + 1):length(potential_predictors)) {
formula <- as.formula(paste("Price ~", potential_predictors[i], "+", potential_predictors[j]))
model_results[counter,] <- c(paste(potential_predictors[i], "+", potential_predictors[j]), build_and_assess_model(formula, data = laptop_data))
counter <- counter + 1
}
}
# A list of potential predictors (modify as needed based on your data)
potential_predictors <- c("Ram", "Weight", "SSD", "Ppi")
# Remove the dependent variable ('Price') from the list
potential_predictors <- potential_predictors[!potential_predictors == "Price"]
# Function to build a model, check assumptions, and store results (modify as needed)
build_and_assess_model <- function(formula, data) {
model <- lm(formula, data = data)
# Add assumption checks here using 'plot(model)'...
return(c(model$r.squared, AIC(model), BIC(model)))
}
# Empty container to store model results
model_results <- matrix(nrow = choose(length(potential_predictors), 2),
ncol = 4,
dimnames = list(NULL, c("Combination", "Adj. R-squared", "AIC", "BIC")))
# Loop through combinations of two predictors
counter <- 1
for (i in 1:(length(potential_predictors) - 1)) {
for (j in (i + 1):length(potential_predictors)) {
formula <- as.formula(paste("Price ~", potential_predictors[i], "+", potential_predictors[j]))
model_results[counter,] <- c(paste(potential_predictors[i], "+", potential_predictors[j]), build_and_assess_model(formula, data = laptop_data))
counter <- counter + 1
}
}
# b) Fit a multivariate linear regression model with two (numerical) independent variables. Choose the most significant regression
# model with two predictors. (Transform the variables if it is needed and test all the assumptions.)
# Then compare this model to the simple linear regression model that you fit in (a). Which one is a better model? Why? (25p)
# Assuming 'laptop_data' has your data and 'Price' is the dependent variable
install.packages("lmtest", force = TRUE)
install.packages("lmtest", force = TRUE)
